{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Copy Paste LLM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :class:`~metaprompting.base.CopyPasteLLM` class allows you to simulate an LLM API by copy-pasting over prompts\nand responses from/to the standard output/input.\n\nThe script must be called with '--interactive' command line switch to use the\n:class:`~metaprompting.base.CopyPasteLLM` class, otherwise, we define a :class:`DummyLLM` class to simulate\ninteraction for generating the documentation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sys\n\nfrom metaprompting import State, LlmAction, HistoryAction, Conversation, LLM, CopyPasteLLM\n\n\nif \"--interactive\" in sys.argv:\n    interactive = True\n    llm = CopyPasteLLM(\n        auto_copy_paste=True,  # automatically: copy LLM prompt to clipboard; paste response back when clipboard changes\n        instructions=False,  # don't print additional instructions\n    )\nelse:\n    class DummyLLM(LLM):\n\n        def __call__(self, prompt, *args, **kwargs):\n            return f\"HERE BE THE RESPONSE TO THE FOLLOWING PROMPT\\n\\n{prompt}\"\n\n    interactive = False\n    llm = DummyLLM()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create conversation graph with state nodes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = Conversation()\ninput_state, history_state, inner_speech_state, output_state = graph.add_states([State(), State(), State(), State()])\ngraph.input_state = input_state\ngraph.output_state = output_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create and connect action nodes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# remember history\nhistory_action = HistoryAction()\ngraph.connect_action([input_state, output_state], history_action, history_state, add=True)\n\n# generate inner speech\ninner_speech_action = LlmAction(llm=llm, prompt_parts=[\n    \"Here is the history of a conversation between Person 1 and Person 2:\\n\\n\",\n    \"What are some general thoughts about this conversation?\\n\\n\" +\n    \"Keep the output short and to a single paragraph of text-only without formatting, bullet points etc\",\n])\ngraph.connect_action(history_state, inner_speech_action, inner_speech_state, add=True)\n\n# construct prompt for response\nresponse_action = LlmAction(llm=llm, prompt_parts=[\n    \"Here is the history of a conversation between Person 1 and Person 2:\\n\\n\",\n    \"\\n\\nSome general thoughts about this conversation are:\\n\\n\",\n    \"\\n\\nThe most recent message from Person 1 is:\\n\\n\",\n    \"\\n\\nWhat could Person 2 reply? Only print the reply itself, nothing else!\",\n])\ngraph.connect_action([history_state, inner_speech_state, input_state], response_action, output_state, add=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialise nodes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inner_speech_state.update(\"This is the beginning of the conversation...\")\ninner_speech_action.block(1)  # block trigger from updating history\nhistory_state.update(\"BEGINNING OF HISTORY\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run conversation interleaved with inner speech\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if interactive:\n    # for running in terminal with '--interactive' switch\n    def print_inner_speech():\n        print(f\"Inner speech: {inner_speech_state.value}\")\n    print(\"Start a conversation (use Ctrl-C to cancel)!\")\n    graph.run(post_response_callback=print_inner_speech)\nelse:\n    # for generating example in documentation\n    input_state.update(\"Some user input...\")\n    print(\"========================\")\n    print(\"User Input\")\n    print(\"========================\")\n    print(input_state.value)\n    print(\"========================\")\n    print(\"LLM Response\")\n    print(\"========================\")\n    print(output_state.value)\n    print(\"========================\")\n    print(\"Inner Speech\")\n    print(\"========================\")\n    print(inner_speech_state.value)\n    print(\"========================\")\n    print(\"History\")\n    print(\"========================\")\n    print(history_state.value)\n    print(\"========================\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}