
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_copy_paste_llm.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_copy_paste_llm.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_copy_paste_llm.py:


Copy Paste LLM
==============================

.. GENERATED FROM PYTHON SOURCE LINES 7-13

The :class:`~metaprompting.base.CopyPasteLLM` class allows you to simulate an LLM API by copy-pasting over prompts
and responses from/to the standard output/input.

The script must be called with '--interactive' command line switch to use the
:class:`~metaprompting.base.CopyPasteLLM` class, otherwise, we define a :class:`DummyLLM` class to simulate
interaction for generating the documentation.

.. GENERATED FROM PYTHON SOURCE LINES 13-34

.. code-block:: Python


    import sys

    from metaprompting import State, LlmAction, HistoryAction, Conversation, LLM, CopyPasteLLM


    if "--interactive" in sys.argv:
        interactive = True
        llm = CopyPasteLLM(
            auto_copy_paste=True,  # automatically: copy LLM prompt to clipboard; paste response back when clipboard changes
            instructions=False,  # don't print additional instructions
        )
    else:
        class DummyLLM(LLM):

            def __call__(self, prompt, *args, **kwargs):
                return f"HERE BE THE RESPONSE TO THE FOLLOWING PROMPT\n\n{prompt}"

        interactive = False
        llm = DummyLLM()








.. GENERATED FROM PYTHON SOURCE LINES 35-36

Create conversation graph with state nodes

.. GENERATED FROM PYTHON SOURCE LINES 36-42

.. code-block:: Python


    graph = Conversation()
    input_state, history_state, inner_speech_state, output_state = graph.add_states([State(), State(), State(), State()])
    graph.input_state = input_state
    graph.output_state = output_state








.. GENERATED FROM PYTHON SOURCE LINES 43-44

Create and connect action nodes

.. GENERATED FROM PYTHON SOURCE LINES 44-66

.. code-block:: Python


    # remember history
    history_action = HistoryAction()
    graph.connect_action([input_state, output_state], history_action, history_state, add=True)

    # generate inner speech
    inner_speech_action = LlmAction(llm=llm, prompt_parts=[
        "Here is the history of a conversation between Person 1 and Person 2:\n\n",
        "What are some general thoughts about this conversation?\n\n" +
        "Keep the output short and to a single paragraph of text-only without formatting, bullet points etc",
    ])
    graph.connect_action(history_state, inner_speech_action, inner_speech_state, add=True)

    # construct prompt for response
    response_action = LlmAction(llm=llm, prompt_parts=[
        "Here is the history of a conversation between Person 1 and Person 2:\n\n",
        "\n\nSome general thoughts about this conversation are:\n\n",
        "\n\nThe most recent message from Person 1 is:\n\n",
        "\n\nWhat could Person 2 reply? Only print the reply itself, nothing else!",
    ])
    graph.connect_action([history_state, inner_speech_state, input_state], response_action, output_state, add=True)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <metaprompting.action.LlmAction object at 0x7fa5d492ccd0>



.. GENERATED FROM PYTHON SOURCE LINES 67-68

Initialise nodes

.. GENERATED FROM PYTHON SOURCE LINES 68-73

.. code-block:: Python


    inner_speech_state.update("This is the beginning of the conversation...")
    inner_speech_action.block(1)  # block trigger from updating history
    history_state.update("BEGINNING OF HISTORY\n\n")








.. GENERATED FROM PYTHON SOURCE LINES 74-75

Run conversation interleaved with inner speech

.. GENERATED FROM PYTHON SOURCE LINES 75-102

.. code-block:: Python


    if interactive:
        # for running in terminal with '--interactive' switch
        def print_inner_speech():
            print(f"Inner speech: {inner_speech_state.value}")
        print("Start a conversation (use Ctrl-C to cancel)!")
        graph.run(post_response_callback=print_inner_speech)
    else:
        # for generating example in documentation
        input_state.update("Some user input...")
        print("========================")
        print("User Input")
        print("========================")
        print(input_state.value)
        print("========================")
        print("LLM Response")
        print("========================")
        print(output_state.value)
        print("========================")
        print("Inner Speech")
        print("========================")
        print(inner_speech_state.value)
        print("========================")
        print("History")
        print("========================")
        print(history_state.value)
        print("========================")




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ========================
    User Input
    ========================
    Some user input...
    ========================
    LLM Response
    ========================
    HERE BE THE RESPONSE TO THE FOLLOWING PROMPT

    Here is the history of a conversation between Person 1 and Person 2:

    BEGINNING OF HISTORY



    Some general thoughts about this conversation are:

    This is the beginning of the conversation...

    The most recent message from Person 1 is:

    Some user input...

    What could Person 2 reply? Only print the reply itself, nothing else!
    ========================
    Inner Speech
    ========================
    HERE BE THE RESPONSE TO THE FOLLOWING PROMPT

    Here is the history of a conversation between Person 1 and Person 2:

    Person 1: Some user input...

    Person 2: HERE BE THE RESPONSE TO THE FOLLOWING PROMPT

    Here is the history of a conversation between Person 1 and Person 2:

    BEGINNING OF HISTORY



    Some general thoughts about this conversation are:

    This is the beginning of the conversation...

    The most recent message from Person 1 is:

    Some user input...

    What could Person 2 reply? Only print the reply itself, nothing else!

    What are some general thoughts about this conversation?

    Keep the output short and to a single paragraph of text-only without formatting, bullet points etc
    ========================
    History
    ========================
    Person 1: Some user input...

    Person 2: HERE BE THE RESPONSE TO THE FOLLOWING PROMPT

    Here is the history of a conversation between Person 1 and Person 2:

    BEGINNING OF HISTORY



    Some general thoughts about this conversation are:

    This is the beginning of the conversation...

    The most recent message from Person 1 is:

    Some user input...

    What could Person 2 reply? Only print the reply itself, nothing else!


    ========================





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.003 seconds)


.. _sphx_glr_download_auto_examples_plot_copy_paste_llm.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_copy_paste_llm.ipynb <plot_copy_paste_llm.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_copy_paste_llm.py <plot_copy_paste_llm.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_copy_paste_llm.zip <plot_copy_paste_llm.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
